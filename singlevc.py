# -*- coding: utf-8 -*-
"""SingleVC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KycgExaFPzacJV-h1X8M1fVkZwmUd0BJ
"""

# sudo apt-get install p7zip
# 7za e /content/SingleVC_pretrained.7z

# gdown --id 1yV9cCne7piqBI9vng13JDdLuRlMkTbZR
# gdown --id 1qpgI41wNXFcH-iKq1Y42JlBC9j0je8PW



!git clone https://github.com/BrightGu/SingleVC

x = ['audioread==2.1.9', 
'editdistance==0.5.3',
'ffmpeg==1.4',
'importlib-metadata==4.5.0', 
'inflect==5.3.0', 
'librosa==0.8.1', 
'matplotlib==3.4.2', 
'numpy==1.20.2', 
'pillow>=8.3.2', 
'pyparsing==2.4.7', 
'pyrubberband==0.3.0', 
'pysoundfile==0.9.0.post1', 
# 'pytorch==1.8.1', 
'pyyaml==5.4.1', 
'scikit-learn==0.24.2', 
'scipy==1.6.3', 
'soundfile==0.10.3.post1', 
'tensorboard==2.0.0', 
'tensorboardx==2.2', 
'torchaudio==0.8.1', 
'torchvision==0.9.1', 
'transformers==4.6.1']

with open('/content/SingleVC/requirements.txt', 'w') as f:
  for i in x:
    f.write(i+"\n")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/SingleVC/

pip install -r requirements.txt





python -m pip install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html
# !pip install https://download.pytorch.org/whl/cu101/torch-1.4.0-cp38-cp38-win_amd64.whl
# !pip install https://download.pytorch.org/whl/cu101/torchvision-0.5.0-cp38-cp38-win_amd64.whl

nvidia-smi

# ########################################## SINGLEVC  ##########################
# test_wav_dir: "SingleVC_Infer/"
# out_dir: "infer_output/"
# resume: True
# resume_model_path: "infer/checkpoint-3000.pt"
# local_rank: 1
# step_record_time: 8
# ########################################## HIFI-GAN ##########################
# hifi_model_path: "hifivoice/pretrained/UNIVERSAL_V1/g_02500000"
# hifi_config_path: "hifivoice/pretrained/UNIVERSAL_V1/config.json"

# ########################################## MEL-FEATURE ##########################
# num_mels: 80
# num_freq: 1025
# n_fft: 1024
# hop_size: 256
# win_size: 1024
# sampling_rate: 22050
# fmin: 0
# fmax: 8000
# num_workers: 1



from __future__ import absolute_import, division, print_function, unicode_literals

import glob
import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '0'
# import sys
# sys.path.append("../")

import numpy as np
import argparse
import json
import torch
from scipy.io.wavfile import write
from hifivoice.env import AttrDict
from hifivoice.meldataset import MAX_WAV_VALUE
from hifivoice.models import Generator

h = None
device = None
checkpoint_file = '/content/g_02500000'
checkpoint_config = '/content/config.json'
with open(checkpoint_config) as f:
    data = f.read()
json_config = json.loads(data)
h = AttrDict(json_config)
if torch.cuda.is_available():
#     torch.cuda.manual_seed(h.seed)
  device = torch.device('cuda')
else:
  device = torch.device('cpu')
  #  torch.device('cpu')
generator = Generator(h).to(device)
state_dict_g = load_checkpoint('/content/g_02500000',device )
generator.load_state_dict(state_dict_g['generator'])
generator.eval()
generator.remove_weight_norm()
print("inference (a,h")

def load_checkpoint(filepath, device):
    assert os.path.isfile(filepath)
    print("Loading '{}'".format(filepath))
    checkpoint_dict = torch.load(filepath, map_location=device)
    print("Complete.")
    return checkpoint_dict


def scan_checkpoint(cp_dir, prefix):
    pattern = os.path.join(cp_dir, prefix + '*')
    cp_list = glob.glob(pattern)
    if len(cp_list) == 0:
        return ''
    return sorted(cp_list)[-1]


def inference(checkpoint_file,input_mels_list,output_dir, h):
  #   if torch.cuda.is_available():
  # #     torch.cuda.manual_seed(h.seed)
  #     device = torch.device('cuda')
  #   else:
  #     device = torch.device('cpu')
  #   generator = Generator(h).to(device)
  #   state_dict_g = load_checkpoint(checkpoint_file,  torch.device('cpu'))
  #   generator.load_state_dict(state_dict_g['generator'])
  #   generator.eval()
  #   generator.remove_weight_norm()
  #   print("inference (a,h")
    with torch.no_grad():
        for file_name, input_mel in input_mels_list:
            x = input_mel
            x = torch.FloatTensor(x).to(device)
            y_g_hat = generator(x)
            audio = y_g_hat.squeeze()
            audio = audio.cpu().numpy()
            output_file = os.path.join(output_dir, file_name + '_generated_e2e.wav')
            write(output_file, h.sampling_rate, audio)
            print(output_file)



def hifi_infer(input_mels_list,output_dir,hifi_model_path,hifi_config_path):
    print('Initializing Inference Process..')
    # parser = argparse.ArgumentParser()
    # # parser.add_argument('--input_mels_dir', default='')
    # parser.add_argument('--input_mels_list', type=str, nargs="+",default="")
    # parser.add_argument('--output_dir', default='')
    # parser.add_argument('--checkpoint_file', default="hifivoice/pretrained/UNIVERSAL_V1/g_02500000")
    # parser.add_argument('--checkpoint_config', default="hifivoice/pretrained/UNIVERSAL_V1/config.json")
    # a = parser.parse_args()

    # a.input_mels_dir = input_mels_dir
    input_mels_list = input_mels_list
    output_dir = output_dir
    checkpoint_file = hifi_model_path
    checkpoint_config = hifi_config_path
    with open(checkpoint_config) as f:
        data = f.read()
    
    global h
    json_config = json.loads(data)
    h = AttrDict(json_config)

    torch.manual_seed(h.seed)
    global device
    if torch.cuda.is_available():
        torch.cuda.manual_seed(h.seed)
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')

    inference(hifi_model_path,input_mels_list,output_dir,h)


# if __name__ == '__main__':
#     main()



import pdb
import os
# os.environ['CUDA_VISIBLE_DEVICES'] = '1'
import sys
sys.path.append("/content/SingleVC/")
import torch
from torch.backends import cudnn
from torch.utils.data import DataLoader
import numpy as np
import yaml
import time
from any2one import util
from any2one.meldataset import Test_MelDataset, get_dataset_filelist,mel_denormalize
from any2one.model.any2one import Generator
# from hifivoice.inference_e2e import  hifi_infer

class Solver():
	def __init__(self, config):
		super(Solver, self).__init__()
		self.config = config
		self.local_rank = self.config['local_rank']
		self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
		self.make_records()
		self.Generator = Generator().to(self.device)
		self.init_epoch = 0
		if self.config['resume']:
			self.resume_model(self.config['resume_model_path'])
		self.logging.info('config = %s', self.config)
		print('param Generator size = %fM ' % (util.count_parameters_in_M(self.Generator)))

	def make_records(self):
		time_record = time.strftime("%Y_%m_%d_%H_%M_%S", time.localtime(time.time()))
		self.log_dir = os.path.join(self.config['out_dir'], time_record, "log")
		self.convt_mel_dir = os.path.join(self.config['out_dir'], time_record, "infer", "mel")
		self.convt_voice_dir = os.path.join(self.config['out_dir'], time_record, "infer", "voice")
		os.makedirs(self.log_dir, exist_ok=True)
		os.makedirs(self.convt_mel_dir, exist_ok=True)
		os.makedirs(self.convt_voice_dir, exist_ok=True)
		self.logging = util.Logger(self.log_dir, "log.txt")

	def get_test_data_loaders(self):
		try:
			os.rmdir('/content/test/.ipynb_checkpoints')
		except:
			pass
		test_filelist = get_dataset_filelist(self.config["test_wav_dir"])
		# testfilelist=[]
		# for f in get_dataset_filelist(self.config["test_wav_dir"]):
		# 				if ".wav" in f:
		# 					testfilelist.append(f)
		testset = Test_MelDataset(test_filelist, self.config["n_fft"],self.config["num_mels"],
							 self.config["hop_size"], self.config["win_size"], self.config["sampling_rate"],self.config["fmin"],
							 self.config["fmax"], device=self.device)
		test_data_loader = DataLoader(testset, num_workers=1, shuffle=False, sampler=None,
									  batch_size=1, pin_memory=False, drop_last=True)
		return test_data_loader

	def resume_model(self, resume_model_path):
		checkpoint_file = resume_model_path
		self.logging.info('loading the model from %s' % (checkpoint_file))
		checkpoint = torch.load(checkpoint_file, map_location='cpu')
		self.init_epoch = checkpoint['epoch']
		self.Generator.load_state_dict(checkpoint['Generator'])

	def infer(self):
		# infer  prepare
		test_data_loader = self.get_test_data_loaders()
		self.Generator.eval()
		self.Generator.remove_weight_norm()
		mel_npy_file_list=[]
		
		with torch.no_grad():
			for idx, (input_mel, word) in enumerate(test_data_loader):
					input_mel = input_mel.cuda()
					fake_mel = self.Generator(input_mel,None)
					fake_mel = torch.clamp(fake_mel, min=0, max=1)
					fake_mel = mel_denormalize(fake_mel)
					fake_mel = fake_mel.transpose(1,2)
					fake_mel = fake_mel.detach().cpu().numpy()
					file_name = "epoch"+"_"+word[0]
					mel_npy_file = os.path.join(self.convt_mel_dir, file_name+ '.npy')
					# pdb.set_trace()
					np.save(mel_npy_file, fake_mel, allow_pickle=False)
					mel_npy_file_list.append([file_name,fake_mel])
					if len(mel_npy_file_list)==500 or idx == len(test_data_loader)-1:
						self.logging.info('【infer_%d】 len: %d', idx,len(mel_npy_file_list))
						hifi_infer(mel_npy_file_list, self.convt_voice_dir,self.config["hifi_model_path"],self.config["hifi_config_path"])
						mel_npy_file_list.clear()

if __name__ == '__main__':
	cudnn.benchmark = True
	config_path = r"/content/SingleVC/any2one/infer/infer_config.yaml"
	with open(config_path) as f:
		config = yaml.load(f, Loader=yaml.Loader)
	config['hifi_model_path']='/content/g_02500000'
	config['hifi_model_path']='/content/SingleVC/g_02500000'

	config['resume_model_path']="/content/checkpoint-3000.pt"
	config['hifi_config_path']= "/content/config.json"
	config['test_wav_dir']='/content/test'
	solver = Solver(config)
	solver.infer()

import IPython.display as ipd
import librosa
fpath='infer_output/2022_01_14_13_18_38/infer/voice/epoch_WhatsApp_Audio_generated_e2e.wav'
# fpath = 'infer_output/2022_01_14_13_18_38/infer/voice/epoch_rec01_edited_2_generated_e2e.wav'
# fpath = 'infer_output/2022_01_14_13_18_38/infer/voice/epoch_rec01_edited_generated_e2e.wav'
fpath = 'infer_output/2022_01_14_13_21_03/infer/voice/epoch_seven_sec_generated_e2e.wav'
# fpath = 'infer_output/2022_01_14_13_22_57/infer/voice/epoch_rec01_generated_e2e.wav'
fpath ='infer_output/2022_01_14_13_45_39/infer/voice/epoch_girl52_generated_e2e.wav'
fpath ='infer_output/2022_01_14_13_45_39/infer/voice/epoch_boy18_generated_e2e.wav'
audio , sampling_rate = librosa.load(str(fpath))
ipd.Audio(audio, rate=sampling_rate ) # load a NumPy array









